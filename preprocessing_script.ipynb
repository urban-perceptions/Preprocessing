{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f502a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "import regex as re\n",
    "import torch\n",
    "import torchtext\n",
    "import torchdata\n",
    "import portalocker\n",
    "from wordsegment import load, segment\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fc16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_fix(h):\n",
    "    \"\"\"\n",
    "    Extracts hashtags from a tweet\n",
    "    \"\"\"\n",
    "    h1 = re.sub(r'[0-9]+', '', h)\n",
    "    h2 = re.sub(r'#', '', h1)\n",
    "    h3 = segment(str(h2))\n",
    "    h4 = ' '.join(map(str, h3)) \n",
    "    return h4\n",
    "\n",
    "# Inputs: dataframe with the tweets and the column with the hashtags\n",
    "def hash_dict(df,hash_col):\n",
    "    \"\"\"\n",
    "    Creates a hashtag dictionary mapping concatenated hashtag\n",
    "    to segmented words\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create a datafame of all hashtags in a column and their counts\n",
    "    # Note: hashtags are in lists inside a cell e.g. [#hash1, #hash2] \n",
    "    tag_counts = df[hash_col].apply(pd.Series).stack().value_counts().to_frame()\n",
    "    tag_counts = tag_counts.reset_index()\n",
    "    tag_counts.columns = ['hash','freq']\n",
    "    # Remove numbers and segment multiple words using hash fix\n",
    "    tag_counts = tag_counts.assign(clean_tag = tag_counts.hash.apply(lambda x: hash_fix(x)))\n",
    "    # Create a dictionary of the hashtags and their clean strings\n",
    "    tag_counts.set_index('hash', inplace=True)\n",
    "    tag_dict = tag_counts['clean_tag'].to_dict()\n",
    "    return tag_dict\n",
    "\n",
    "def preprocess_tweet_hash(row):\n",
    "    \"\"\"\n",
    "    preprocesses a tweet\n",
    "    \"\"\"\n",
    "    p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION,\n",
    "              p.OPT.RESERVED, p.OPT.SMILEY, p.OPT.NUMBER)\n",
    "    try: \n",
    "        text = row['text_hash_split'].replace(\"&amp\", \"and\")\n",
    "        text = p.clean(text)\n",
    "    except AttributeError: \n",
    "        return None\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_tweet_no_hash(row):\n",
    "    \"\"\"\n",
    "    preprocesses a tweet\n",
    "    \n",
    "    \"\"\"\n",
    "    p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION,\n",
    "              p.OPT.RESERVED, p.OPT.SMILEY, p.OPT.NUMBER, p.OPT.HASHTAG)\n",
    "    text = row['text'].replace(\"&amp\", \"and\")\n",
    "    text = p.clean(text)\n",
    "    return text\n",
    "\n",
    "def clean_city(c, df, text_column):\n",
    "    \"\"\"\n",
    "    cleans tweets corresponding to a certain city\n",
    "    \"\"\"\n",
    "    print(\"beginning city: \", c)\n",
    "    if c is not None:\n",
    "        tweets_analysis = df.loc[(df.loc[:, \"city\"] == c), :]\n",
    "    else: \n",
    "        tweets_analysis = df\n",
    "    #tweets_analysis[\"hashtag\"] = tweets_analysis[text_column].apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "    tweets_analysis['hashtag'] = tweets_analysis[text_column].str.findall(r\"#(\\w+)\")\n",
    "    hashtags = []\n",
    "    for row in tweets_analysis[\"hashtag\"]:\n",
    "        try:\n",
    "            for h in row: \n",
    "                hashtags.append(h)\n",
    "        except TypeError:\n",
    "            continue\n",
    "    hashtags = pd.DataFrame(hashtags)\n",
    "    hashtags.rename(columns = {0: \"hashtags\"}, inplace = True)\n",
    "    hashtags[\"hashtags\"] = \"#\"+hashtags[\"hashtags\"]\n",
    "    tweets_analysis[\"text_hash_split\"] = tweets_analysis[text_column]\n",
    "    print(\"replacing hashtags\")\n",
    "    tag_dict = hash_dict(hashtags,'hashtags')\n",
    "    print(\"made hash dictionary\")\n",
    "    #print(tag_dict)\n",
    "    tweets_analysis.astype({\"text_hash_split\": str})\n",
    "    #tweets_analysis.text_hash_split.as_type(str)\n",
    "    print(\"beginning replacement process\")\n",
    "    tweets_analysis.text_hash_split.replace(tag_dict, regex=True, inplace= True)\n",
    "    #print(tweets_analysis[\"text_hash_split\"])\n",
    "    print(\"done with hashtags\")\n",
    "    tweets_analysis['text_with_hash'] = tweets_analysis.apply(preprocess_tweet_hash, axis=1)\n",
    "    #print(tweets_analysis[\"text_with_hash\"])\n",
    "    tweets_analysis['text_with_hash'] = tweets_analysis['text_with_hash'\n",
    "                                                       ].str.lower().str.replace('[^\\w\\s]',\n",
    "                                                                                 ' ').str.replace('\\s\\s+', ' ')\n",
    "    return tweets_analysis\n",
    "    \n",
    "\n",
    "def clean_cities(input_filename, output_filename): \n",
    "    \"\"\"\n",
    "    cleans entire file \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_filename)\n",
    "    final_array = []\n",
    "    cities = df.city.unique()\n",
    "    for c in cities:\n",
    "        dfc = df.copy()\n",
    "        try: \n",
    "            new_array = clean_city(c, dfc, \"text\")\n",
    "        except KeyError: \n",
    "            continue\n",
    "        final_array.append(new_array)\n",
    "    final_cities = pd.concat(final_array)\n",
    "    final_cities.rename(columns = {\"text_with_hash\": \"text_clean\"}, inplace = True)\n",
    "    final_cities.to_csv(output_filename)\n",
    "    return final_cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashtags = cleaning(\"city_tweets_large.csv\", \"text\", \"city_tweets_hashtags_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dcebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cities = clean_cities(\"city_tweets_hashtags2.csv\", \"city_tweets_hashtags2_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984fa39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning tweets that are not \n",
    "\n",
    "final_tweets = pd.DataFrame()\n",
    "final_array = []\n",
    "#cities = df.city.unique()\n",
    "for i in range(30):\n",
    "    beginning_index = i*2000\n",
    "    end_index = i*2000 + 2000\n",
    "    print(\"beginning at\", beginning_index)\n",
    "    print(\"ending at\", end_index)\n",
    "    dfc = new_tweets.copy()\n",
    "    new_array = clean_city(None, dfc.loc[beginning_index: end_index, :], \"text\")\n",
    "    final_array.append(new_array)\n",
    "final_tweets = pd.concat(final_array)\n",
    "final_tweets.rename(columns = {\"text_with_hash\": \"text_clean\"}, inplace = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
